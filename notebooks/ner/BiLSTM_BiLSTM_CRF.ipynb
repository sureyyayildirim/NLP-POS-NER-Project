{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/teropa/nlp.git\n",
        "!ls nlp/resources/corpora/conll2002\n"
      ],
      "metadata": {
        "id": "ZuAEWv2mrZ-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = \"nlp/resources/corpora/conll2002\"\n",
        "os.chdir(DATA_DIR)\n",
        "\n",
        "!tar -xzf esp.tgz\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu8nDc0iraAs",
        "outputId": "87f44578-0e43-44d7-bc4a-eed04eae5778"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): esp.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "esp.testa  esp.testb  esp.train  ned.testa  ned.testb  ned.train  README\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content\")\n",
        "CONLL_DIR = \"/content/nlp/resources/corpora/conll2002\"\n"
      ],
      "metadata": {
        "id": "r-R70RxstY7k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll2002(path):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    sentences.append(tokens)\n",
        "                    labels.append(tags)\n",
        "                    tokens = []\n",
        "                    tags = []\n",
        "                continue\n",
        "\n",
        "            parts = line.split()\n",
        "            tokens.append(parts[0])\n",
        "            tags.append(parts[-1])\n",
        "\n",
        "    if tokens:\n",
        "        sentences.append(tokens)\n",
        "        labels.append(tags)\n",
        "\n",
        "    return sentences, labels\n"
      ],
      "metadata": {
        "id": "Gc8tnTOeraDF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONLL_DIR = \"/content/nlp/resources/corpora/conll2002\"\n",
        "\n",
        "train_file = f\"{CONLL_DIR}/esp.train\"\n",
        "dev_file   = f\"{CONLL_DIR}/esp.testa\"\n",
        "test_file  = f\"{CONLL_DIR}/esp.testb\"\n",
        "\n",
        "X_train_tokens, y_train_bio = read_conll2002(train_file)\n",
        "X_dev_tokens,   y_dev_bio   = read_conll2002(dev_file)\n",
        "X_test_tokens,  y_test_bio  = read_conll2002(test_file)\n",
        "\n",
        "print(len(X_train_tokens), len(X_dev_tokens), len(X_test_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CZFOmS8raFW",
        "outputId": "b817f974-b0d2-43f8-c28f-617784731e34"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8323 1915 1517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_tokens[0])\n",
        "print(y_train_bio[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJcNijljraI-",
        "outputId": "91267ff8-c8b8-49b3-87f2-0be347a2d4ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n",
            "['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = set()\n",
        "for seq in (y_train_bio + y_dev_bio + y_test_bio):\n",
        "    all_labels.update(seq)\n",
        "\n",
        "labels = sorted(all_labels)\n",
        "label2id = {lab: i for i, lab in enumerate(labels)}\n",
        "id2label = {i: lab for lab, i in label2id.items()}\n",
        "\n",
        "labels, label2id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVZpOEqVtf_3",
        "outputId": "8225c2f8-3f7c-4076-fc20-cc165198cfb1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['B-LOC',\n",
              "  'B-MISC',\n",
              "  'B-ORG',\n",
              "  'B-PER',\n",
              "  'I-LOC',\n",
              "  'I-MISC',\n",
              "  'I-ORG',\n",
              "  'I-PER',\n",
              "  'O'],\n",
              " {'B-LOC': 0,\n",
              "  'B-MISC': 1,\n",
              "  'B-ORG': 2,\n",
              "  'B-PER': 3,\n",
              "  'I-LOC': 4,\n",
              "  'I-MISC': 5,\n",
              "  'I-ORG': 6,\n",
              "  'I-PER': 7,\n",
              "  'O': 8})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 100\n",
        "\n",
        "all_train_tokens = [w for s in X_train_tokens for w in s]\n",
        "vocab_words = sorted(set(all_train_tokens))\n",
        "\n",
        "word2idx = {w: i+2 for i, w in enumerate(vocab_words)}\n",
        "word2idx[\"<PAD>\"] = 0\n",
        "word2idx[\"<UNK>\"] = 1\n",
        "\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n"
      ],
      "metadata": {
        "id": "TLzn8U3mtgCE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentences(tokens_list, labels_list, word2idx, label2id, max_len):\n",
        "    X_ids = []\n",
        "    y_ids = []\n",
        "    for tokens, labels in zip(tokens_list, labels_list):\n",
        "        x = [word2idx.get(w, 1) for w in tokens]\n",
        "        y = [label2id[t] for t in labels]\n",
        "        X_ids.append(x)\n",
        "        y_ids.append(y)\n",
        "\n",
        "    X_pad = pad_sequences(X_ids, maxlen=max_len, padding=\"post\", truncating=\"post\", value=0)\n",
        "    y_pad = pad_sequences(y_ids, maxlen=max_len, padding=\"post\", truncating=\"post\", value=label2id[\"O\"])\n",
        "\n",
        "    return np.array(X_pad), np.array(y_pad)\n",
        "\n",
        "X_train_lstm, y_train_lstm = encode_sentences(X_train_tokens, y_train_bio, word2idx, label2id, MAX_LEN)\n",
        "X_dev_lstm,   y_dev_lstm   = encode_sentences(X_dev_tokens,   y_dev_bio,   word2idx, label2id, MAX_LEN)\n",
        "X_test_lstm,  y_test_lstm  = encode_sentences(X_test_tokens,  y_test_bio,  word2idx, label2id, MAX_LEN)\n"
      ],
      "metadata": {
        "id": "GOvHMie4tgEd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(X_padded):\n",
        "    return (X_padded != 0).astype(\"float32\")\n",
        "\n",
        "train_mask = create_mask(X_train_lstm)\n",
        "dev_mask   = create_mask(X_dev_lstm)\n",
        "test_mask  = create_mask(X_test_lstm)\n"
      ],
      "metadata": {
        "id": "S4o9_hr-tgGZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "def decode_with_mask(y_true_ids, y_pred_ids, mask, id2label):\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    for t_seq, p_seq, m_seq in zip(y_true_ids, y_pred_ids, mask):\n",
        "        t_list = []\n",
        "        p_list = []\n",
        "        for t, p, m in zip(t_seq, p_seq, m_seq):\n",
        "            if m == 0:\n",
        "                continue\n",
        "            t_list.append(id2label[int(t)])\n",
        "            p_list.append(id2label[int(p)])\n",
        "        true_tags.append(t_list)\n",
        "        pred_tags.append(p_list)\n",
        "\n",
        "    return true_tags, pred_tags\n"
      ],
      "metadata": {
        "id": "8zPQU2r6tgJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, TimeDistributed, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "EMB_DIM    = 150\n",
        "HIDDEN_DIM = 200\n",
        "\n",
        "num_words  = len(word2idx)\n",
        "num_labels = len(labels)\n",
        "\n",
        "inputs = Input(shape=(MAX_LEN,), name=\"input_ids\")\n",
        "\n",
        "x = Embedding(\n",
        "    input_dim=num_words,\n",
        "    output_dim=EMB_DIM,\n",
        "    mask_zero=True,\n",
        "    name=\"word_embeddings\"\n",
        ")(inputs)\n",
        "\n",
        "x = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "outputs = TimeDistributed(\n",
        "    Dense(num_labels, activation=\"softmax\")\n",
        ")(x)\n",
        "\n",
        "bilstm_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "bilstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),       # lr 1e-3 → 5e-4\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "bilstm_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "yMRgZvlmt8KS",
        "outputId": "634a434e-dc8a-452b-f02e-b367f6735520"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ word_embeddings     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m150\u001b[0m)  │  \u001b[38;5;34m3,915,150\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m400\u001b[0m)  │    \u001b[38;5;34m561,600\u001b[0m │ word_embeddings[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m400\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m9\u001b[0m)    │      \u001b[38;5;34m3,609\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ word_embeddings     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,915,150</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">561,600</span> │ word_embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,609</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,480,359\u001b[0m (17.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,480,359</span> (17.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,480,359\u001b[0m (17.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,480,359</span> (17.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"bilstm_best.weights.h5\",\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "history_bilstm = bilstm_model.fit(\n",
        "    X_train_lstm,\n",
        "    y_train_lstm,\n",
        "    sample_weight=train_mask,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_dev_lstm, y_dev_lstm),\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVkTPXZtt8PJ",
        "outputId": "cf1330f6-7c82-4656-bf1e-be68aa6781bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 29ms/step - accuracy: 0.9491 - loss: 0.7187 - val_accuracy: 0.9751 - val_loss: 0.3079\n",
            "Epoch 2/20\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 27ms/step - accuracy: 0.9810 - loss: 0.1902 - val_accuracy: 0.9817 - val_loss: 0.2379\n",
            "Epoch 3/20\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 26ms/step - accuracy: 0.9902 - loss: 0.1023 - val_accuracy: 0.9837 - val_loss: 0.2117\n",
            "Epoch 4/20\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - accuracy: 0.9948 - loss: 0.0557 - val_accuracy: 0.9844 - val_loss: 0.2198\n",
            "Epoch 5/20\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - accuracy: 0.9963 - loss: 0.0398 - val_accuracy: 0.9850 - val_loss: 0.2226\n",
            "Epoch 6/20\n",
            "\u001b[1m521/521\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - accuracy: 0.9975 - loss: 0.0274 - val_accuracy: 0.9848 - val_loss: 0.2532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "\n",
        "y_pred_probs = bilstm_model.predict(X_test_lstm)\n",
        "y_pred_ids   = y_pred_probs.argmax(axis=-1)\n",
        "\n",
        "\n",
        "def decode_with_mask(y_true_ids, y_pred_ids, mask, id2label):\n",
        "    all_true, all_pred = [], []\n",
        "    for t_seq, p_seq, m_seq in zip(y_true_ids, y_pred_ids, mask):\n",
        "        t_tags, p_tags = [], []\n",
        "        for t, p, m in zip(t_seq, p_seq, m_seq):\n",
        "            if m == 0:  # PAD\n",
        "                continue\n",
        "            t_tags.append(id2label[int(t)])\n",
        "            p_tags.append(id2label[int(p)])\n",
        "        all_true.append(t_tags)\n",
        "        all_pred.append(p_tags)\n",
        "    return all_true, all_pred\n",
        "\n",
        "true_tags_bilstm, pred_tags_bilstm = decode_with_mask(\n",
        "    y_test_lstm, y_pred_ids, test_mask, id2label\n",
        ")\n",
        "\n",
        "print(\"BiLSTM classification report:\")\n",
        "print(classification_report(true_tags_bilstm, pred_tags_bilstm, digits=4))\n",
        "print(\"BiLSTM F1:\", f1_score(true_tags_bilstm, pred_tags_bilstm))\n",
        "\n",
        "\n",
        "# --- MISC hariç F1 ---\n",
        "\n",
        "def remove_misc(true_tags, pred_tags):\n",
        "    new_true, new_pred = [], []\n",
        "    for t_seq, p_seq in zip(true_tags, pred_tags):\n",
        "        t2, p2 = [], []\n",
        "        for t, p in zip(t_seq, p_seq):\n",
        "            if \"MISC\" in t:\n",
        "                continue\n",
        "            t2.append(t)\n",
        "            p2.append(p)\n",
        "        new_true.append(t2)\n",
        "        new_pred.append(p2)\n",
        "    return new_true, new_pred\n",
        "\n",
        "true_no_misc, pred_no_misc = remove_misc(true_tags_bilstm, pred_tags_bilstm)\n",
        "\n",
        "print(\"\\nBiLSTM (MISC hariç) classification report:\")\n",
        "print(classification_report(true_no_misc, pred_no_misc, digits=4))\n",
        "print(\"BiLSTM F1 (no MISC):\", f1_score(true_no_misc, pred_no_misc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ey_O2mt8RU",
        "outputId": "a7082e86-32c3-4436-87e1-0b0622c3d323"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
            "BiLSTM classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.6890    0.6800    0.6845      1072\n",
            "        MISC     0.3171    0.3441    0.3300       340\n",
            "         ORG     0.7252    0.7210    0.7231      1398\n",
            "         PER     0.7712    0.7198    0.7446       721\n",
            "\n",
            "   micro avg     0.6799    0.6720    0.6760      3531\n",
            "   macro avg     0.6256    0.6163    0.6206      3531\n",
            "weighted avg     0.6843    0.6720    0.6779      3531\n",
            "\n",
            "BiLSTM F1: 0.6759720837487538\n",
            "\n",
            "BiLSTM (MISC hariç) classification report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.7225    0.6800    0.7006      1072\n",
            "        MISC     0.0000    0.0000    0.0000         0\n",
            "         ORG     0.7545    0.7210    0.7374      1398\n",
            "         PER     0.7828    0.7198    0.7500       721\n",
            "\n",
            "   micro avg     0.7103    0.7070    0.7087      3191\n",
            "   macro avg     0.5649    0.5302    0.5470      3191\n",
            "weighted avg     0.7501    0.7070    0.7279      3191\n",
            "\n",
            "BiLSTM F1 (no MISC): 0.7086539971729229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "PAD_IDX = word2idx[\"<PAD>\"]\n",
        "\n",
        "class NERDatasetTorch(Dataset):\n",
        "    def __init__(self, X, y, mask):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        self.mask = torch.tensor(mask.astype(bool))  # bool mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.X[idx],\n",
        "            \"labels\": self.y[idx],\n",
        "            \"mask\": self.mask[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = NERDatasetTorch(X_train_lstm, y_train_lstm, train_mask)\n",
        "dev_dataset   = NERDatasetTorch(X_dev_lstm,   y_dev_lstm,   dev_mask)\n",
        "test_dataset  = NERDatasetTorch(X_test_lstm,  y_test_lstm,  test_mask)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II9q_cCWt8Ut",
        "outputId": "31f631cf-c182-4794-ac20-dbb6b89c00ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class CRF(nn.Module):\n",
        "    def __init__(self, num_tags):\n",
        "        super().__init__()\n",
        "        self.num_tags = num_tags\n",
        "\n",
        "        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n",
        "        self.end_transitions   = nn.Parameter(torch.empty(num_tags))\n",
        "        self.transitions       = nn.Parameter(torch.empty(num_tags, num_tags))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Daha geniş aralık: CRF geçişleri daha güçlü\n",
        "        nn.init.uniform_(self.start_transitions, -0.4, 0.4)\n",
        "        nn.init.uniform_(self.end_transitions,   -0.4, 0.4)\n",
        "        nn.init.uniform_(self.transitions,       -0.4, 0.4)\n",
        "\n",
        "    def forward(self, emissions, tags, mask):\n",
        "        log_num = self._score_sentence(emissions, tags, mask)\n",
        "        log_den = self._compute_log_partition(emissions, mask)\n",
        "        nll = -(log_num - log_den)\n",
        "        return nll.mean()\n",
        "\n",
        "    def _compute_log_partition(self, emissions, mask):\n",
        "        batch_size, seq_len, num_tags = emissions.size()\n",
        "        alpha = self.start_transitions + emissions[:, 0]  # (batch, num_tags)\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            emit_t = emissions[:, t]\n",
        "            mask_t = mask[:, t].unsqueeze(1)\n",
        "\n",
        "            score_t = alpha.unsqueeze(2) + self.transitions + emit_t.unsqueeze(1)\n",
        "            new_alpha = torch.logsumexp(score_t, dim=1)\n",
        "\n",
        "            alpha = torch.where(mask_t, new_alpha, alpha)\n",
        "\n",
        "        alpha = alpha + self.end_transitions\n",
        "        return torch.logsumexp(alpha, dim=1)\n",
        "\n",
        "    def _score_sentence(self, emissions, tags, mask):\n",
        "        batch_size, seq_len, num_tags = emissions.size()\n",
        "\n",
        "        first_tag = tags[:, 0]\n",
        "        score = self.start_transitions[first_tag]\n",
        "        score = score + emissions[:, 0].gather(1, first_tag.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            mask_t   = mask[:, t]\n",
        "            prev_tag = tags[:, t-1]\n",
        "            curr_tag = tags[:, t]\n",
        "\n",
        "            trans_score = self.transitions[prev_tag, curr_tag]\n",
        "            emit_score  = emissions[:, t].gather(1, curr_tag.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            score = score + (trans_score + emit_score) * mask_t\n",
        "\n",
        "        seq_lengths = mask.long().sum(dim=1) - 1\n",
        "        last_tags   = tags.gather(1, seq_lengths.unsqueeze(1)).squeeze(1)\n",
        "        score = score + self.end_transitions[last_tags]\n",
        "\n",
        "        return score\n",
        "\n",
        "    def decode(self, emissions, mask):\n",
        "        batch_size, seq_len, num_tags = emissions.size()\n",
        "\n",
        "        score = self.start_transitions + emissions[:, 0]\n",
        "        history = []\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            emit_t = emissions[:, t]\n",
        "            mask_t = mask[:, t].unsqueeze(1)\n",
        "\n",
        "            score_t = score.unsqueeze(2) + self.transitions + emit_t.unsqueeze(1)\n",
        "            best_score_t, best_path_t = score_t.max(dim=1)\n",
        "\n",
        "            score = torch.where(mask_t, best_score_t, score)\n",
        "            history.append(best_path_t)\n",
        "\n",
        "        score = score + self.end_transitions\n",
        "        best_last_score, best_last_tag = score.max(dim=1)\n",
        "\n",
        "        best_paths = []\n",
        "        for i in range(batch_size):\n",
        "            seq_len_i = mask[i].sum().item()\n",
        "            last_tag = best_last_tag[i].item()\n",
        "            path = [last_tag]\n",
        "\n",
        "            for hist_t in reversed(history[: seq_len_i-1]):\n",
        "                last_tag = hist_t[i][last_tag].item()\n",
        "                path.append(last_tag)\n",
        "\n",
        "            path.reverse()\n",
        "            best_paths.append(path)\n",
        "\n",
        "        return best_paths\n"
      ],
      "metadata": {
        "id": "P_zJnBut0Y4w"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = word2idx[\"<PAD>\"]\n",
        "\n",
        "class NERDatasetTorch(Dataset):\n",
        "    def __init__(self, X, y, mask):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        # CRF için bool mask\n",
        "        self.mask = torch.tensor(mask.astype(bool))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.X[idx],\n",
        "            \"labels\": self.y[idx],\n",
        "            \"mask\": self.mask[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = NERDatasetTorch(X_train_lstm, y_train_lstm, train_mask)\n",
        "dev_dataset   = NERDatasetTorch(X_dev_lstm,   y_dev_lstm,   dev_mask)\n",
        "test_dataset  = NERDatasetTorch(X_test_lstm,  y_test_lstm,  test_mask)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "A5Izrio50ZCb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMCRF(nn.Module):\n",
        "    def __init__(self, vocab_size, num_labels, pad_idx, emb_dim=100, hidden_dim=100):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, num_labels)\n",
        "        self.crf = CRF(num_labels)\n",
        "\n",
        "    def forward(self, input_ids, tags=None, mask=None):\n",
        "        embeds = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        emissions = self.hidden2tag(lstm_out)\n",
        "\n",
        "        if tags is not None:\n",
        "            # training: loss\n",
        "            loss = self.crf(emissions, tags, mask)\n",
        "            return loss\n",
        "        else:\n",
        "            # inference: decoded paths\n",
        "            pred_paths = self.crf.decode(emissions, mask)\n",
        "            return pred_paths\n"
      ],
      "metadata": {
        "id": "kgccgXa495rP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB_DIM    = 150\n",
        "HIDDEN_DIM = 200\n",
        "\n",
        "class BiLSTMCRF(nn.Module):\n",
        "    def __init__(self, vocab_size, num_labels, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, EMB_DIM, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=EMB_DIM,\n",
        "            hidden_size=HIDDEN_DIM,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.hidden2tag = nn.Linear(HIDDEN_DIM * 2, num_labels)\n",
        "        self.crf = CRF(num_labels)\n",
        "\n",
        "    def forward(self, input_ids, tags=None, mask=None):\n",
        "        embeds = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        emissions = self.hidden2tag(lstm_out)\n",
        "\n",
        "        if tags is not None:\n",
        "            loss = self.crf(emissions, tags, mask)\n",
        "            return loss\n",
        "        else:\n",
        "            return self.crf.decode(emissions, mask)\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "num_labels = len(labels)\n",
        "\n",
        "bilstm_crf_model = BiLSTMCRF(\n",
        "    vocab_size=vocab_size,\n",
        "    num_labels=num_labels,\n",
        "    pad_idx=PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(bilstm_crf_model.parameters(), lr=5e-4)\n"
      ],
      "metadata": {
        "id": "9Bb69cPi95tv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "def train_epoch_crf(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels    = batch[\"labels\"].to(device)\n",
        "        mask      = batch[\"mask\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_ids, tags=labels, mask=mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        del input_ids, labels, mask, loss\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate_ner_crf(model, dataloader, device, id2label):\n",
        "    model.eval()\n",
        "    all_true, all_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels    = batch[\"labels\"].to(device)\n",
        "            mask      = batch[\"mask\"].to(device)\n",
        "\n",
        "            pred_paths = model(input_ids, tags=None, mask=mask)\n",
        "\n",
        "            for true_ids, pred_ids, m in zip(labels, pred_paths, mask):\n",
        "                seq_len_i = m.sum().item()\n",
        "                true_seq  = true_ids[:seq_len_i].cpu().numpy()\n",
        "\n",
        "                t_tags = [id2label[int(t)] for t in true_seq]\n",
        "                p_tags = [id2label[int(p)] for p in pred_ids]\n",
        "\n",
        "                all_true.append(t_tags)\n",
        "                all_pred.append(p_tags)\n",
        "\n",
        "    f1 = f1_score(all_true, all_pred)\n",
        "    return f1, all_true, all_pred\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "patience = 2\n",
        "best_dev_f1 = 0.0\n",
        "pat = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_epoch_crf(bilstm_crf_model, train_loader, optimizer, device)\n",
        "    dev_f1, _, _ = evaluate_ner_crf(bilstm_crf_model, dev_loader, device, id2label)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} - train loss: {train_loss:.4f}  dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "    if dev_f1 > best_dev_f1:\n",
        "        best_dev_f1 = dev_f1\n",
        "        pat = 0\n",
        "        torch.save(bilstm_crf_model.state_dict(), \"bilstm_crf_best.pt\")\n",
        "        print(\"  -> new best model saved\")\n",
        "    else:\n",
        "        pat += 1\n",
        "        if pat >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4mpib0i95v9",
        "outputId": "f1229cbd-f66e-40ea-c563-c853e34dbf7c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - train loss: 13.0164  dev F1: 0.4423\n",
            "  -> new best model saved\n",
            "Epoch 2/10 - train loss: 6.0020  dev F1: 0.5599\n",
            "  -> new best model saved\n",
            "Epoch 3/10 - train loss: 3.5574  dev F1: 0.6247\n",
            "  -> new best model saved\n",
            "Epoch 4/10 - train loss: 2.1516  dev F1: 0.6386\n",
            "  -> new best model saved\n",
            "Epoch 5/10 - train loss: 1.2506  dev F1: 0.6578\n",
            "  -> new best model saved\n",
            "Epoch 6/10 - train loss: 0.7165  dev F1: 0.6617\n",
            "  -> new best model saved\n",
            "Epoch 7/10 - train loss: 0.4275  dev F1: 0.6628\n",
            "  -> new best model saved\n",
            "Epoch 8/10 - train loss: 0.2870  dev F1: 0.6630\n",
            "  -> new best model saved\n",
            "Epoch 9/10 - train loss: 0.2209  dev F1: 0.6607\n",
            "Epoch 10/10 - train loss: 0.1810  dev F1: 0.6607\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# En iyi ağırlıkları geri yükle\n",
        "bilstm_crf_model.load_state_dict(torch.load(\"bilstm_crf_best.pt\", map_location=device))\n",
        "print(\"Best BiLSTM-CRF weights loaded.\")\n",
        "\n",
        "test_f1, test_true_tags, test_pred_tags = evaluate_ner_crf(\n",
        "    bilstm_crf_model, test_loader, device, id2label\n",
        ")\n",
        "\n",
        "print(\"BiLSTM-CRF classification report:\")\n",
        "print(classification_report(test_true_tags, test_pred_tags, digits=4))\n",
        "print(\"BiLSTM-CRF F1:\", test_f1)\n",
        "\n",
        "\n",
        "# --- MISC hariç F1 ---\n",
        "\n",
        "def remove_misc(true_tags, pred_tags):\n",
        "    new_true, new_pred = [], []\n",
        "    for t_seq, p_seq in zip(true_tags, pred_tags):\n",
        "        t2, p2 = [], []\n",
        "        for t, p in zip(t_seq, p_seq):\n",
        "            if \"MISC\" in t:\n",
        "                continue\n",
        "            t2.append(t)\n",
        "            p2.append(p)\n",
        "        new_true.append(t2)\n",
        "        new_pred.append(p2)\n",
        "    return new_true, new_pred\n",
        "\n",
        "true_no_misc_crf, pred_no_misc_crf = remove_misc(test_true_tags, test_pred_tags)\n",
        "\n",
        "print(\"\\nBiLSTM-CRF (MISC hariç) classification report:\")\n",
        "print(classification_report(true_no_misc_crf, pred_no_misc_crf, digits=4))\n",
        "print(\"BiLSTM-CRF F1 (no MISC):\", f1_score(true_no_misc_crf, pred_no_misc_crf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tYc_8ak95ye",
        "outputId": "6035ff76-246d-4308-9fcc-a1ef2cd4f393"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best BiLSTM-CRF weights loaded.\n",
            "BiLSTM-CRF classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.8051    0.6511    0.7200      1072\n",
            "        MISC     0.3902    0.4441    0.4154       340\n",
            "         ORG     0.7637    0.6888    0.7243      1398\n",
            "         PER     0.7187    0.7157    0.7172       721\n",
            "\n",
            "   micro avg     0.7201    0.6593    0.6884      3531\n",
            "   macro avg     0.6694    0.6249    0.6442      3531\n",
            "weighted avg     0.7311    0.6593    0.6918      3531\n",
            "\n",
            "BiLSTM-CRF F1: 0.6883500887049083\n",
            "\n",
            "BiLSTM-CRF (MISC hariç) classification report:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.8173    0.6511    0.7248      1072\n",
            "        MISC     0.0000    0.0000    0.0000         0\n",
            "         ORG     0.7829    0.6888    0.7329      1398\n",
            "         PER     0.7247    0.7157    0.7202       721\n",
            "\n",
            "   micro avg     0.7300    0.6822    0.7053      3191\n",
            "   macro avg     0.5812    0.5139    0.5445      3191\n",
            "weighted avg     0.7813    0.6822    0.7273      3191\n",
            "\n",
            "BiLSTM-CRF F1 (no MISC): 0.7053296614288028\n"
          ]
        }
      ]
    }
  ]
}